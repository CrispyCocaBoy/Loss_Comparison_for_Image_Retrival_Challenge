{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model 2\n",
    "Implement the model without the implementation of the weight."
   ],
   "id": "be3d3cce8ee3ed03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:26:18.647701Z",
     "start_time": "2025-05-19T19:26:18.640016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\"data_example_rota/train\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, image_folder_dataset):\n",
    "        self.transform = image_folder_dataset.transform\n",
    "        self.class_to_paths = defaultdict(list)\n",
    "        self.data = []\n",
    "\n",
    "        for path, class_idx in image_folder_dataset.imgs:\n",
    "            class_name = image_folder_dataset.classes[class_idx]\n",
    "            self.class_to_paths[class_name].append(path)\n",
    "            self.data.append((class_name, path))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        cls, anchor_path = self.data[index]\n",
    "        positive_path = random.choice([p for p in self.class_to_paths[cls] if p != anchor_path])\n",
    "        negative_cls = random.choice([c for c in self.class_to_paths if c != cls])\n",
    "        negative_path = random.choice(self.class_to_paths[negative_cls])\n",
    "\n",
    "        def load_img(p): return self.transform(Image.open(p).convert(\"RGB\"))\n",
    "\n",
    "        return load_img(anchor_path), load_img(positive_path), load_img(negative_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "triplet_dataset = TripletDataset(train_dataset)\n",
    "triplet_loader = DataLoader(triplet_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class SimpleImageDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform):\n",
    "        self.image_paths = list(Path(folder_path).glob(\"*.jpg\"))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        return img, self.image_paths[idx].name\n",
    "\n",
    "query_dataset = SimpleImageDataset(\"data_example_rota/test/query\", transform)\n",
    "gallery_dataset = SimpleImageDataset(\"data_example_rota/test/gallery\", transform)\n",
    "\n",
    "query_loader = DataLoader(query_dataset, batch_size=1, shuffle=False)\n",
    "gallery_loader = DataLoader(gallery_dataset, batch_size=32, shuffle=False)\n",
    "\n"
   ],
   "id": "406fd8002e8803da",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:21:55.466065Z",
     "start_time": "2025-05-19T19:21:55.462894Z"
    }
   },
   "cell_type": "code",
   "source": "len(gallery_dataset)",
   "id": "74af9e7b60411b45",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:11:49.818030Z",
     "start_time": "2025-05-19T19:11:49.815512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setting the Dataloader\n",
    "# We prepeare the data for the ML also setting the first hyperparmeter batch size\n",
    "batch_size = 32\n",
    "\n",
    "## Training data\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "## Test data\n",
    "query_loader = DataLoader(query_dataset, batch_size=batch_size, shuffle=False)\n",
    "gallery_loader = DataLoader(gallery_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "31f5d64902ea90f8",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:11:51.090738Z",
     "start_time": "2025-05-19T19:11:51.087292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "class ResNetEmbedder(nn.Module):\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        # Carica ResNet-50 pre-addestrata\n",
    "        self.backbone = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        in_features = self.backbone.fc.in_features\n",
    "\n",
    "        # Rimuove il classificatore originale\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        # Proiezione nello spazio degli embedding\n",
    "        self.embedding = nn.Linear(in_features, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)           # output: [batch_size, 2048]\n",
    "        x = self.embedding(x)          # output: [batch_size, embedding_dim]\n",
    "        x = F.normalize(x, p=2, dim=1) # L2-normalizzazione\n",
    "        return x\n"
   ],
   "id": "c6bde2241205cdc8",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:18:17.925047Z",
     "start_time": "2025-05-19T19:12:58.546095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Triplet loss\n",
    "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
    "    pos_dist = F.pairwise_distance(anchor, positive, p=2)\n",
    "    neg_dist = F.pairwise_distance(anchor, negative, p=2)\n",
    "    return F.relu(pos_dist - neg_dist + margin).mean()\n",
    "\n",
    "\n",
    "# Inizializza modello\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNetEmbedder(embedding_dim=128).to(device)\n",
    "\n",
    "# Compila il modello per ottimizzazioni (solo se torch >= 2.0)\n",
    "try:\n",
    "    model = torch.compile(model, backend=\"aot_eager\")\n",
    "    print(\"Modello compilato con torch.compile()\")\n",
    "except AttributeError:\n",
    "    print(\"torch.compile non disponibile (serve PyTorch >= 2.0)\")\n",
    "\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train() #metto il modello\n",
    "    running_loss = 0.0\n",
    "    for anchors, positives, negatives in tqdm(triplet_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        anchors = anchors.to(device)\n",
    "        positives = positives.to(device)\n",
    "        negatives = negatives.to(device)\n",
    "\n",
    "        anchor_emb = model(anchors)\n",
    "        positive_emb = model(positives)\n",
    "        negative_emb = model(negatives)\n",
    "\n",
    "        loss = triplet_loss(anchor_emb, positive_emb, negative_emb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Avg Loss: {running_loss / len(triplet_loader):.4f}\")\n"
   ],
   "id": "b29c5108a1fa1b99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello compilato con torch.compile()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/3 [00:00<?, ?it/s]/Users/matteomassari/Documents/university/Magistrale/1° Anno/2° Semestre/Introduction to Machine Learning/IML_competitions/.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:749: RuntimeWarning: Guards may run slower on Python 3.13.0. Consider upgrading to Python 3.13.1+.\n",
      "  warnings.warn(\n",
      "Epoch 1/10:  67%|██████▋   | 2/3 [00:30<00:15, 15.08s/it]/Users/matteomassari/Documents/university/Magistrale/1° Anno/2° Semestre/Introduction to Machine Learning/IML_competitions/.venv/lib/python3.13/site-packages/torch/_dynamo/guards.py:749: RuntimeWarning: Guards may run slower on Python 3.13.0. Consider upgrading to Python 3.13.1+.\n",
      "  warnings.warn(\n",
      "Epoch 1/10: 100%|██████████| 3/3 [00:36<00:00, 12.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.7499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 3/3 [00:36<00:00, 12.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 0.5194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 3/3 [00:35<00:00, 11.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 0.2668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 3/3 [00:30<00:00, 10.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Loss: 0.1043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 3/3 [00:29<00:00,  9.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Loss: 0.0657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 3/3 [00:31<00:00, 10.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Avg Loss: 0.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 3/3 [00:30<00:00, 10.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Loss: 0.0486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 3/3 [00:29<00:00,  9.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Avg Loss: 0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 3/3 [00:29<00:00,  9.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Avg Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 3/3 [00:29<00:00,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Avg Loss: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:19:46.572041Z",
     "start_time": "2025-05-19T19:19:46.568593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_embeddings(dataloader, model, device):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    filenames = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, names in dataloader:\n",
    "            images = images.to(device)\n",
    "            embs = model(images)\n",
    "            embeddings.append(embs.cpu())\n",
    "            filenames.extend(names)\n",
    "\n",
    "    embeddings = torch.cat(embeddings)\n",
    "    return embeddings, filenames"
   ],
   "id": "bbc33bbd1c6c2456",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:33:06.435940Z",
     "start_time": "2025-05-19T19:33:06.400019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(query_loader, gallery_embeddings, gallery_paths, model, device, top_k=5, mapping_file=None):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    # Carica il mapping query → immagini corrette\n",
    "    if mapping_file:\n",
    "        with open(mapping_file, 'r') as f:\n",
    "            query_mapping = {os.path.basename(entry[\"filename\"]): set(os.path.basename(p) for p in entry[\"gallery_images\"])\n",
    "                             for entry in json.load(f)}\n",
    "    else:\n",
    "        query_mapping = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(tqdm(query_loader, desc=\"Evaluating queries\")):\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Estrai embeddings del batch\n",
    "            batch_embeddings = model(images)\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                query_embedding = batch_embeddings[i]\n",
    "                query_filename = os.path.basename(labels[i]) if isinstance(labels[i], str) else os.path.basename(labels[i][0])\n",
    "\n",
    "                # Similarità con tutta la gallery\n",
    "                similarities = F.cosine_similarity(query_embedding.unsqueeze(0), gallery_embeddings)\n",
    "\n",
    "                # Ottieni top-k più simili\n",
    "                top_indices = similarities.topk(top_k).indices\n",
    "                top_paths = [os.path.basename(gallery_paths[idx]) for idx in top_indices]\n",
    "\n",
    "                # Verifica immagini corrette secondo mapping\n",
    "                true_gallery = query_mapping.get(query_filename, set())\n",
    "                correct_count = sum([img in true_gallery for img in top_paths])\n",
    "                total_true = len(true_gallery)\n",
    "\n",
    "                print(f\"Query #{len(results)+1} - {query_filename}\")\n",
    "                for j, path in enumerate(top_paths):\n",
    "                    print(f\"\\tTop {j+1}: {path}\")\n",
    "                print(f\"\\tCorrect: {correct_count} / {total_true}\")\n",
    "\n",
    "                results.append({\n",
    "                    \"query\": query_filename,\n",
    "                    \"top_k\": top_paths,\n",
    "                    \"correct\": correct_count,\n",
    "                    \"total_true\": total_true\n",
    "                })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "evaluate(\n",
    "    model=model,\n",
    "    query_loader=query_loader,\n",
    "    gallery_loader=gallery_loader,\n",
    "    mapping_file=\"data_example_rota/query_to_gallery_mapping.json\",\n",
    "    device=device,\n",
    "    topk=3\n",
    ")\n"
   ],
   "id": "24d2d6dbb62a5fd6",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "evaluate() got an unexpected keyword argument 'gallery_loader'. Did you mean 'query_loader'?",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[59]\u001B[39m\u001B[32m, line 58\u001B[39m\n\u001B[32m     48\u001B[39m                 results.append({\n\u001B[32m     49\u001B[39m                     \u001B[33m\"\u001B[39m\u001B[33mquery\u001B[39m\u001B[33m\"\u001B[39m: query_filename,\n\u001B[32m     50\u001B[39m                     \u001B[33m\"\u001B[39m\u001B[33mtop_k\u001B[39m\u001B[33m\"\u001B[39m: top_paths,\n\u001B[32m     51\u001B[39m                     \u001B[33m\"\u001B[39m\u001B[33mcorrect\u001B[39m\u001B[33m\"\u001B[39m: correct_count,\n\u001B[32m     52\u001B[39m                     \u001B[33m\"\u001B[39m\u001B[33mtotal_true\u001B[39m\u001B[33m\"\u001B[39m: total_true\n\u001B[32m     53\u001B[39m                 })\n\u001B[32m     55\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[32m---> \u001B[39m\u001B[32m58\u001B[39m \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     59\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     60\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquery_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mquery_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     61\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgallery_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgallery_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     62\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmapping_file\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdata_example_rota/query_to_gallery_mapping.json\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     63\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     64\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtopk\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3\u001B[39;49m\n\u001B[32m     65\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/university/Magistrale/1° Anno/2° Semestre/Introduction to Machine Learning/IML_competitions/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mTypeError\u001B[39m: evaluate() got an unexpected keyword argument 'gallery_loader'. Did you mean 'query_loader'?"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:26:27.661227Z",
     "start_time": "2025-05-19T19:26:27.515727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, (img, label) in enumerate(query_loader):\n",
    "    print(f\"Query #{i} - shape: {img.shape}, label: {label}\")"
   ],
   "id": "89808b1459bafbfb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query #0 - shape: torch.Size([1, 3, 224, 224]), label: ('n01855672_10973.jpg',)\n",
      "Query #1 - shape: torch.Size([1, 3, 224, 224]), label: ('000002.jpg',)\n"
     ]
    }
   ],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
