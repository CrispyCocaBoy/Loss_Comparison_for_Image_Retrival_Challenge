{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-15T13:37:49.151922Z",
     "start_time": "2025-05-15T13:37:49.142362Z"
    }
   },
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.samples = []  # lista di tuple (path, label)\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        # Scansione ricorsiva\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in sorted(files):\n",
    "                if file.lower().endswith(('.jpg', '.png')):\n",
    "                    path = os.path.join(root, file)\n",
    "                    # Estrae la label dal nome della sottocartella\n",
    "                    label = os.path.basename(os.path.dirname(path))\n",
    "                    self.samples.append((path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        return image, label, path\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.6431, 0.6431, 0.6431,  ..., 0.6471, 0.6471, 0.6471],\n",
       "          [0.6431, 0.6431, 0.6431,  ..., 0.6471, 0.6471, 0.6471],\n",
       "          [0.6431, 0.6431, 0.6431,  ..., 0.6471, 0.6471, 0.6471],\n",
       "          ...,\n",
       "          [0.6157, 0.5961, 0.5882,  ..., 0.6471, 0.6588, 0.6510],\n",
       "          [0.6039, 0.5804, 0.5647,  ..., 0.6667, 0.6863, 0.6863],\n",
       "          [0.5961, 0.5686, 0.5529,  ..., 0.6745, 0.6980, 0.7020]],\n",
       " \n",
       "         [[0.7137, 0.7137, 0.7137,  ..., 0.7137, 0.7137, 0.7137],\n",
       "          [0.7137, 0.7137, 0.7137,  ..., 0.7137, 0.7137, 0.7137],\n",
       "          [0.7137, 0.7137, 0.7137,  ..., 0.7137, 0.7137, 0.7137],\n",
       "          ...,\n",
       "          [0.4745, 0.4510, 0.4392,  ..., 0.4235, 0.4157, 0.3922],\n",
       "          [0.4627, 0.4353, 0.4157,  ..., 0.4431, 0.4392, 0.4235],\n",
       "          [0.4549, 0.4235, 0.4000,  ..., 0.4510, 0.4549, 0.4392]],\n",
       " \n",
       "         [[0.8000, 0.8000, 0.8000,  ..., 0.7922, 0.7922, 0.7922],\n",
       "          [0.8000, 0.8000, 0.8000,  ..., 0.7922, 0.7922, 0.7922],\n",
       "          [0.8000, 0.8000, 0.8000,  ..., 0.7922, 0.7922, 0.7922],\n",
       "          ...,\n",
       "          [0.2863, 0.2627, 0.2471,  ..., 0.1529, 0.1569, 0.1412],\n",
       "          [0.2745, 0.2431, 0.2235,  ..., 0.1725, 0.1804, 0.1765],\n",
       "          [0.2667, 0.2353, 0.2078,  ..., 0.1804, 0.1961, 0.1922]]]),\n",
       " 'antelope',\n",
       " './data_example/training/antelope/03d7fc0888.jpg')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T13:48:08.971216Z",
     "start_time": "2025-05-15T13:48:08.962702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_dataset = CustomImageDataset(\"./data_example/training\")\n",
    "training_dataset[0]\n",
    "\n",
    "dataloader = DataLoader(training_dataset, batch_size=32, shuffle=True, num_workers=4)"
   ],
   "id": "815cb90a3fb1b1b5",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T13:58:02.127422Z",
     "start_time": "2025-05-15T13:58:02.124418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
    "    pos_dist = F.pairwise_distance(anchor, positive, p=2)\n",
    "    neg_dist = F.pairwise_distance(anchor, negative, p=2)\n",
    "    loss = torch.clamp(pos_dist - neg_dist + margin, min=0.0)\n",
    "    return loss.mean()\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base = resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(base.children())[:-1])  # rimuovi classificatore\n",
    "        self.fc = nn.Linear(512, 128)  # embedding finale\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)   # [B, 512, 1, 1]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return F.normalize(x, p=2, dim=1)  # normalizza l'embedding\n",
    "import random\n",
    "\n",
    "class TripletDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.label_to_indices = self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        label_to_indices = {}\n",
    "        for idx, (_, label, _) in enumerate(self.base_dataset):\n",
    "            label_to_indices.setdefault(label, []).append(idx)\n",
    "        return label_to_indices\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        anchor_img, anchor_label, _ = self.base_dataset[index]\n",
    "\n",
    "        # Positive sample (stessa classe)\n",
    "        positive_idx = index\n",
    "        while positive_idx == index:\n",
    "            positive_idx = random.choice(self.label_to_indices[anchor_label])\n",
    "        positive_img, _, _ = self.base_dataset[positive_idx]\n",
    "\n",
    "        # Negative sample (classe diversa)\n",
    "        negative_label = random.choice([l for l in self.label_to_indices.keys() if l != anchor_label])\n",
    "        negative_idx = random.choice(self.label_to_indices[negative_label])\n",
    "        negative_img, _, _ = self.base_dataset[negative_idx]\n",
    "\n",
    "        return anchor_img, positive_img, negative_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n"
   ],
   "id": "9af272e8de3cd376",
   "outputs": [],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
