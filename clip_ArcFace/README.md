# ViT-B/32 with Cross-Entropy + ArcFace

This repository contains the source code for training and evaluating a **CLIP ViT-B/32** model enhanced with a **custom classification head and ArcFace loss** for the task of **face image retrieval**.

## Overview

We fine-tune a CLIP-based Vision Transformer (ViT-B/32) for image retrieval by combining the **Cross-Entropy Loss** with **Additive Angular Margin** to improve class separability in the embedding space. This approach is motivated by ArcFaceâ€™s effectiveness in face recognition tasks.

The best-performing model in our benchmark used:

* **Backbone**: CLIP ViT-B/32 (visual encoder only)
* **Head**: Linear projection + ArcFace
* **Loss**: Cross-Entropy with Angular Margin (ArcFace)

## Project Structure

```
clip_ArcFace/
â”œâ”€â”€ repository/              # Model checkpoints and outputs
â”‚   â”œâ”€â”€ all_weights/         # All model weights per epoch
â”‚   â”œâ”€â”€ best_model/          # Best model based on val loss
â”‚   â”œâ”€â”€ checkpoints/         # Checkpoints with optimizer state
â”‚   â”œâ”€â”€ metrics/             # Stored metrics during training
â”‚   â””â”€â”€ results/             # Final evaluation outputs
â”‚
â”œâ”€â”€ src/                     # Source code
â”‚   â”œâ”€â”€ data_loading.py      # Dataloaders and transforms
â”‚   â”œâ”€â”€ distance.py          # Similarity functions (cosine)
â”‚   â”œâ”€â”€ loss.py              # Loss functions (ArcFace)
â”‚   â”œâ”€â”€ model.py             # CLIP-based architecture
â”‚   â”œâ”€â”€ test.py              # Evaluation loop (retrieval metrics)
â”‚   â””â”€â”€ train.py             # Training loop with early stopping
â”‚
â”œâ”€â”€ main.py                  # Entry point for training/testing/validation
â”œâ”€â”€ wandb/                   # Weights & Biases logs
â””â”€â”€ README.md                # This file
```

## Training

Run the model with set the model in Main as training = True

This will:
* Load the dataset
  * training
  * validation
  * test
* Start training 
  * tracking train_losses and validation_losses
* Save checkpoints and metrics in `repository/`
  * all_weights: contain the model for each epoch
  * checkpoints: contain the weights for each epoch (allow the resume of the trainings)
  * metrics: contain csv with train_losses and validation_losses

## Testing

Run the model with set the model in Main as training = False
It will test the model in a specific epoch. To perform an overall testing use the other script evaluate_all, that eill crreate a csv that shows epoch and accurancy


Install with:

```bash
pip install -r requirements.txt
```

## ðŸ“Š Results

The ViT-B/32 + ArcFace model demonstrated strong generalization, especially on face-like datasets.

---
